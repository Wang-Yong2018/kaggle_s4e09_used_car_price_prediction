---
title: "used_car_price_predict"
author: "WangYong"
date: "`r Sys.Date()`"
output: html_document
---
## library & load_data 
### library
```{r}
library(tidyverse)
library(tidymodels)
library(future)

library(bonsai)
library(lightgbm)

library(lubridate)
```

### loading data
```{r}
data_path <- '../input/playground-series-s4e9/'
train<- read.csv(file.path(data_path, 'train.csv'))
test <-  read.csv(file.path(data_path, 'test.csv'))
submission <-  read.csv(file.path(data_path, 'sample_submission.csv'))
```

### quick skim
```{r}
train|> skimr::skim()
```


```{r}
test|> skimr::skim()
```


```{r}
submission |> skimr::skim()
```

### check if train & test is same distribution
```{r}
get_df_var<-function(df){
  df|>
    summarize_all(var)|>
    pivot_longer(cols=everything(),
                 names_to='feature',
                 values_to='variance')
  
}
train|>get_df_var()
```


```{r}
get_var_compare <- function(rcp,is_debug=F){
  
  train_var <- rcp|> 
    prep()|>bake(new_data=train)|>
    select(-loan_status, -id) |> # 去除目标变量和ID变量
    summarise_all(var)|>
    pivot_longer(cols = everything(),
               names_to = "feature",
               values_to = "variance")
    
  test_var  <- rcp|>
    prep()|>bake(new_data=test)|>
    select( -id) |> # 去除目标变量和ID变量
    summarise_all(var)|>
    pivot_longer(cols = everything(),
               names_to = "feature",
               values_to = "variance")
  
  compared_result<-
    train_var |> 
    left_join(test_var, by='feature')|>
    mutate(variance_ratio = round(variance.x / variance.y,2))
  
  nfeature_var_changed <- 
    compared_result|>
    filter(variance_ratio >1*1.5 ) |>
    nrow()
 
  if (is_debug){
    print(compared_result)
    
  } 
  return(nfeature_var_changed)
}


```

## coding
### 1. Data Loading and Initial Exploration ----

```{r}
 cars_train <- train
 cars_test <- test 

 glimpse(cars_train) 

 # Check for missing values
 sum(is.na(cars_train)) # Total missing values

 #Quickly check for highly correlated features
 cars_train |>
   select_if(is.numeric) |>
   cor() |>
   as.data.frame() |>
   rownames_to_column("feature1") |>
   pivot_longer(-feature1, names_to = "feature2", values_to = "correlation") |>
   filter(feature1 != feature2, abs(correlation) > 0.7) |>
   arrange(desc(abs(correlation))) |>
   print()
```


### 2. Feature Engineering ----
```{r}
cars_processed <- cars_train |>
   mutate(
     # Example: Age of the car
     age = year(Sys.Date()) - model_year,
     age = ifelse(milage < 0, 0, milage), # Handle potential errors in year
     # Example: Combine Make and Model for more specific categories
     make_model = paste(brand, model, sep = "_"),
     # Example: Log transform Price to reduce skew
     price = log1p(price)
   )

 #Repeat transofmration for test dataset
 cars_test_processed <- cars_test |>
   mutate(
     # Example: Age of the car
     age = year(Sys.Date()) - model_year,
     age = ifelse(milage < 0, 0, milage), # Handle potential errors in year
     # Example: Combine Make and Model for more specific categories
     make_model = paste(brand, model, sep = "_"),
   )
```


### 3. Data Splitting ----
```{r}
set.seed(42)
 cars_split <- initial_split(cars_processed, prop = 0.8, strata = price)
 cars_train_set <- training(cars_split)
 cars_test_set <- testing(cars_split)
cars_folds <- vfold_cv(cars_train_set, v = 5)
```


### 4. Preprocessing Recipe ----
```{r}
cars_recipe <-
   recipe(price ~ ., data = cars_train_set) |>
   update_role(id, new_role = "id") |>
   # Impute numeric missing values with median
   step_impute_median(all_numeric_predictors()) |>
   # Impute categorical missing values with mode
   step_impute_mode(all_nominal_predictors()) |>
   # One-hot encode nominal predictors
   step_dummy(all_nominal_predictors())|>
   #Remove constant features
   step_zv(all_predictors()) |>
   #Normalize data
   step_normalize(all_numeric_predictors())
```


### 5. Model Specification ----
```{r}
lgbm_model <-
   boost_tree(
     trees = 500, # Number of trees
     learn_rate = 0.1,
     loss_reduction = 0.001,
     sample_size = 0.85, # Added sample_size
     tree_depth = tune(),
     mtry = tune(),
     min_n = tune()
   ) |>
   set_engine("lightgbm",
              metric='rmse', 
              # num_leaves = 30,
              num_threads = 12,
              verbose=1) |>
   set_mode("regression")

```


### 6. Workflow ----
```{r}
cars_workflow <-
   workflow() |>
   add_recipe(cars_recipe) |>
   add_model(lgbm_model)
```

### 7. Tuning Grid ----
```{r}
cars_grid <- grid_max_entropy(
  #learn_rate(range = c(0.01, 0.1)),
  # loss_reduction(range = c(0, 10)), #Keep if you want it.
  #bag_fraction(range = c(0.7, 0.9)), # Specify prop = TRUE.
  tree_depth(range = c(5, 10)),
  finalize(mtry(range = c(2, 10)),
           select(cars_train_set, -Price)),
  min_n(range = c(2, 20)),
  size = 10
)

```


### 8. Cross-Validation ----
```{r}
# combined it with step3 data splitting
```


### 9. Tuning and Evaluation ----
```{r}
plan(multisession,workers =4)
cars_tune_results <- cars_workflow |>
  tune_grid(
    resamples = cars_folds,
    grid = cars_grid,
    metrics = metric_set(rmse),
    control = control_grid(save_pred = TRUE, 
                           verbose = TRUE,
                           allow_par = TRUE) # Keep predictions
  )
 
 # Find best parameters
 best_params <- cars_tune_results |>
   select_best("rmse")

 # Finalize workflow with best parameters
 final_workflow <- cars_workflow |>
   finalize_workflow(best_params)
```


```{r}
# Fit the final workflow to the training data
final_lgbm_fit <- last_fit(final_workflow,cars_split )
final_lgbm_mod <- extract_workflow(final_lgbm_fit )
collect_metrics(final_lgmb_mod)

plan(sequential)

```


### 10. Evaluate on Test Set ----
```{r}
cars_predictions <- final_model |>
   predict(new_data = cars_test_set) |>
   bind_cols(cars_test_set |> select(price))

 # Calculate RMSE
 rmse_value <- cars_predictions |>
   rmse(truth = price, estimate = .pred)

 print(rmse_value)

```

### 11. Prepare Submission ----
```{r}
final_predictions <- final_model |>
   predict(new_data = cars_test_processed) |>
   mutate(price = expm1(.pred)) |>
   select(di, price)

 #Handle negative predictions
 final_predictions <- final_predictions |>
   mutate(Price = ifelse(Price < 0, 0, Price))

 # Save submission file
 write_csv(final_predictions, "submission.csv")

```

## kaggle submission
```{r}
# submit latest submission.csv
system('kaggle competitions submit -c playground-series-s4e9 -f submission.csv.gz -m "init _ just sample_submission.csv"')
Sys.sleep(15)
# get latest score 
system('kaggle competitions submissions -q -c playground-series-s4e9')
```

