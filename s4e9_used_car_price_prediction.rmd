---
title: "used_car_price_predict"
author: "WangYong"
date: "`r Sys.Date()`"
output: html_document
---
## library & load_data 
### library
```{r}
library(tidyverse)
library(tidymodels)
library(future)
library(textrecipes)

library(bonsai)
library(lightgbm)

library(lubridate)
```

### loading data
```{r}
data_path <- '../input/playground-series-s4e9/'
train<- read.csv(file.path(data_path, 'train.csv'))
test <-  read.csv(file.path(data_path, 'test.csv'))
submission <-  read.csv(file.path(data_path, 'sample_submission.csv'))
```

### quick skim
```{r}
train|> skimr::skim()
```


```{r}
test|> skimr::skim()
```


```{r}
submission |> skimr::skim()
```

### check if train & test is same distribution
```{r}
get_df_var<-function(df){
  df|>
    summarize_all(var)|>
    pivot_longer(cols=everything(),
                 names_to='feature',
                 values_to='variance')
  
}
train|>get_df_var()
```


```{r}
get_var_compare <- function(rcp,is_debug=F){
  
  train_var <- rcp|> 
    prep()|>bake(new_data=train)|>
    select(-loan_status, -id) |> # 去除目标变量和ID变量
    summarise_all(var)|>
    pivot_longer(cols = everything(),
               names_to = "feature",
               values_to = "variance")
    
  test_var  <- rcp|>
    prep()|>bake(new_data=test)|>
    select( -id) |> # 去除目标变量和ID变量
    summarise_all(var)|>
    pivot_longer(cols = everything(),
               names_to = "feature",
               values_to = "variance")
  
  compared_result<-
    train_var |> 
    left_join(test_var, by='feature')|>
    mutate(variance_ratio = round(variance.x / variance.y,2))
  
  nfeature_var_changed <- 
    compared_result|>
    filter(variance_ratio >1*1.5 ) |>
    nrow()
 
  if (is_debug){
    print(compared_result)
    
  } 
  return(nfeature_var_changed)
}


```

## coding
### 1. Data Loading and Initial Exploration ----



### 2. Feature Engineering ----
- leave it in the preprocessing recipe

### 3. Data Splitting ----
```{r}
set.seed(1234)
df_split <- initial_split(train, prop = 0.8, strata = price)
 train_set <- training(df_split)
 test_set <- testing(df_split)
cv_folds <- vfold_cv(train_set,v = 5)
```


### 4. Preprocessing Recipe ----

#### 4.1 v0 base_line
```{r}
rcp_bs_v0 <-
  recipe(price ~ ., data = train) |>
  update_role(id, new_role = "id") |>
  step_rm(ext_col,int_col)|>
  step_impute_median(all_numeric_predictors())|>
  step_mutate(model_year=2025-model_year,
              milage_year=milage/model_year,
              accident=case_when(
                accident=='None reported'~'none',
                accident=='At least 1 accident or damage reported'~'report',
                accident==''~NA,
                .default='unknow'))|>
  step_mutate(brand= as.factor(brand),
              model= as.factor(model),
              engine= as.factor(engine),
              #ext_col= as.factor(ext_col),
              #int_col= as.factor(int_col),
              transmission= as.factor(transmission),
              clean_title= as.factor(brand),
              accident= as.factor(accident),
              fuel_type=as.factor(fuel_type))|>
  step_novel(all_nominal_predictors())|>
  step_unknown(all_nominal_predictors())|>
  step_other(all_nominal_predictors(),threshold=0.01)|>
  step_dummy(all_nominal_predictors(),one_hot = TRUE)|>
  
  #step_log(price, offset=1, skip=TRUE) |>
  step_log(milage,model_year,milage_year ,offset=1) |>
  
  step_nzv(all_numeric_predictors())|>
  step_normalize(all_numeric_predictors())|>
  step_corr(all_predictors())#|>
  #check_missing(all_predictors())
 
#rcp_bs_v0 |>prep()
  
```
#### 4.2 v1 base_line with engine feature
```{r}
rcp_bs_v1 <- 
   recipe(price ~ ., data = train) |>
  update_role(id, new_role = "id") |>
  step_rm(ext_col,int_col)|>
  step_mutate(
    hp =str_extract(engine, "^(\\d+\\.?\\d*)HP|^(\\d+)HP")|>parse_number())|>
  step_mutate( 
    litre = str_extract(engine, "(\\d+\\.?\\d*)L|(\\d+\\.?\\d*) Liter")|>parse_number(),
    engine_type = str_extract(engine,'Rotary|H6|H4|V6|V-8|V8|V10|V12|W12|I3|I4|I6|\\s[4|5|6|8]\\s| 10 | 12 '),
    is_ecar=case_when(grepl('Electric',engine)~'YES'),
    voltage = str_extract(engine, " (\\d\\d)V")|>parse_number(),
    gdi_type = str_extract(engine, "GDI|PFI|EFI|MPFI|DDI|SIDI"),
    gas_position = str_extract(engine, "OHV|OHC|SOHC|DOHC"),
    cylinder = str_extract(engine, "(\\d+) Cylinder|(\\d+) Cylinder Engine|(\\d+)(?=[^\\d]*Cylinder)"),
    is_turbo = case_when(grepl('Turbo|turbo',engine)~'YES'),
    is_twinturbo= case_when(grepl('Twin|twin',engine)~'YES'),
    fuel_info = str_extract(engine, "Gasoline|Diesel|Electric|Flexible Fuel|Gasoline/Mild Electric Hybrid|Flex Fuel Capability") )|>
  step_rm(engine)|>
  step_impute_median(all_numeric_predictors())|>
  step_mutate(model_year=2025-model_year,
              milage_year=milage/model_year,
              accident=case_when(
                accident=='None reported'~'none',
                accident=='At least 1 accident or damage reported'~'report',
                accident==''~NA,
                .default='unknow'))|>
  step_mutate(brand= as.factor(brand),
              model= as.factor(model),
              #engine= as.factor(engine),
              #ext_col= as.factor(ext_col),
              #int_col= as.factor(int_col),
              transmission= as.factor(transmission),
              clean_title= as.factor(brand),
              accident= as.factor(accident),
              fuel_type=as.factor(fuel_type))|>
   step_novel(all_nominal_predictors())|>
  step_unknown(all_nominal_predictors())|>
  step_other(all_nominal_predictors(),threshold=0.01)|>
  step_dummy(all_nominal_predictors(),one_hot = TRUE)|>
  step_log(price, offset=1, skip=TRUE) |>
  step_log(milage,model_year,milage_year ,offset=1) |>
  
  step_nzv(all_numeric_predictors())|>
  step_normalize(all_numeric_predictors())|>
  step_corr(all_predictors())|>
  check_missing(all_predictors())
# rcp_bs_v1 |>prep() |>bake(new_data = train_set)
```

### 5. Model Specification ----
```{r}
lgbm_eng<-
   boost_tree(
     # trees = 500, # Number of trees
     # learn_rate = 0.1,
     # loss_reduction = 0.001,
     # sample_size = 0.85, # Added sample_size
     # tree_depth = tune(),
     # mtry = tune(),
     # min_n = tune()
   ) |>
   set_mode("regression")|>
   set_engine("lightgbm",
              metric='rmse', 
              # num_leaves = 30,
              num_threads = 4,
              verbose=1) 

lm_eng<-
   linear_reg() |>
   set_mode("regression")|>
   set_engine("lm") 

```


### 6. Workflow ----
#### simple wflow
```{r}
set.seed(1234)
simple_wf_fit <-
  workflow() |>
  add_recipe(rcp_bs_v0) |>
  add_model(lm_eng)|>
  fit_resamples(cv_folds)

simple_wf_fit|>collect_metrics()
simple_pred <- simple_wf_rmse <-
  simple_wf_fit|>
  predict(new_data = train_set)|>
  mutate(.pred=expm1(.pred))

simple_pred|>
  bind_cols( train_set )|>
  rmse(truth=price,estimate=.pred)


simple_wf_rmse
#note: TODO with log 76050/75230, without log 72111, need check
simple_wf_fit |>
  extract_fit_engine()|>
  plot()

```
#### simple workflowset

```{r}
set.seed(1234)
library(future)
plan(multisession,workers = 8)
ctrl <- control_resamples(save_pred = TRUE, save_workflow = TRUE,verbose=TRUE)
wfs_result <-
  workflow_set(preproc = list(bs_v0=rcp_bs_v0,bs_v1=rcp_bs_v1),
               models = list(linear=lm_eng) ) |>
  option_add()|>
  workflow_map(fn='fit_resamples',
               resamples =cv_folds,
               metrics = metric_set(rmse, rsq),
               control = ctrl
               )
wfs_result|>collect_metrics()  
  
plan(sequential)
```
```{r}

```

### 7. Tuning Grid ----
```{r}
cars_grid <- grid_space_filling(
  #learn_rate(range = c(0.01, 0.1)),
  # loss_reduction(range = c(0, 10)), #Keep if you want it.
  #bag_fraction(range = c(0.7, 0.9)), # Specify prop = TRUE.
  tree_depth(range = c(5, 10)),
  finalize(mtry(range = c(2, 10)),
           select(cars_train_set, -Price)),
  min_n(range = c(2, 20)),
  size = 10
)

```


### 8. Cross-Validation ----
```{r}
# combined it with step3 data splitting
```


### 9. Tuning and Evaluation ----
```{r}
# plan(multisession,workers =2)
cars_tune_results <- cars_workflow |>
  tune_grid(
    resamples = cars_folds,
    grid = cars_grid,
    metrics = metric_set(rmse),
     control = control_grid(save_pred = TRUE, 
                            verbose = TRUE,
                            allow_par = F) # Keep predictions
  )
 
 # Find best parameters
 best_params <- cars_tune_results |>
   select_best("rmse")

 # Finalize workflow with best parameters
 final_workflow <- cars_workflow |>
   finalize_workflow(best_params)
```


```{r}
# Fit the final workflow to the training data
final_lgbm_fit <- last_fit(final_workflow,cars_split )
final_lgbm_mod <- extract_workflow(final_lgbm_fit )
collect_metrics(final_lgmb_mod)

# plan(sequential)

```


### 10. Evaluate on Test Set ----
```{r}
cars_predictions <- final_model |>
   predict(new_data = cars_test_set) |>
   bind_cols(cars_test_set |> select(price))

 # Calculate RMSE
 rmse_value <- cars_predictions |>
   rmse(truth = price, estimate = .pred)

 print(rmse_value)

```

### 11. Prepare Submission ----
```{r}
final_predictions <- final_model |>
   predict(new_data = cars_test_processed) |>
   mutate(price = expm1(.pred)) |>
   select(di, price)

 #Handle negative predictions
 final_predictions <- final_predictions |>
   mutate(Price = ifelse(Price < 0, 0, Price))

 # Save submission file
 write_csv(final_predictions, "submission.csv")

```

## kaggle submission
```{r}
# submit latest submission.csv
system('kaggle competitions submit -c playground-series-s4e9 -f submission.csv.gz -m "init _ just sample_submission.csv"')
Sys.sleep(15)
# get latest score 
system('kaggle competitions submissions -q -c playground-series-s4e9')
```

